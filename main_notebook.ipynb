{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 1\n",
    "* CSCI-5931 : Deep Learning\n",
    "* Spring 2024\n",
    "* Instructor: Ashis Kumer Biswas\n",
    "* Student name: Carol Kiekhaefer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run code in a Jupyter notebook cell\n",
    "# % indicates it's a Jupyter magic command; allows me to execute conda commands directly from the notebook interface. \n",
    "# This command installs the pyarrow package into the currently active conda environment\n",
    "#%conda install pyarrow  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Little Background about the problem\n",
    "\n",
    "`Customer churn`` occurs when customers stop doing business with a company, also known as customer attrition. It is also referred to as loss of clients or customers.\n",
    "\n",
    "You are given sensitive information of 9,000 of an European Bank, EBQ. Your task is to build an Artificial Neural Network (ANN) based on the dataset such that later the ANN model can predict correctly who is going to leave next. This predictive analysis is vital for the EBQ bank to revise their business strategy towards customer retention. What do you think?\n",
    "\n",
    "Anyway, you are recruited by the bank to do the data science. And, the head of the bank only trusts heads, i.e., brains…. I mean neural networks for making any decisions. And luckily you were in Dr. B’s class and you know something(?) about the ANN that you could successfully convince the head of the bank during the interview. He has put a lot of faith in you. Now, can you solve his problem?\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Please download the zip file, `PA1-deliverables.zip``. Unzip it in your workspace. Here below is the file hierarchy of \"PA1-deliverables/\" folder:\n",
    "\n",
    "```\n",
    "PA1-deliverables\n",
    "├── 2024-Spring-DL-PA1-assignment.ipynb\n",
    "├── dataset\n",
    "│   └── datasetX.csv\n",
    "├── figures\n",
    "│   ├── le.png\n",
    "│   ├── nn-1.png\n",
    "│   ├── nn-1.svg\n",
    "│   ├── nn-2.png\n",
    "│   ├── nn-2.svg\n",
    "│   ├── nn-3.png\n",
    "│   ├── nn-3.svg\n",
    "│   └── ohe.png\n",
    "└── saved_models\n",
    "```\n",
    "As you can see you will mostly be working with the 2023-Fall-DL-PA1-assignment.ipynb, i.e., the jupyter notebook. The notebook accesses the dataset files: `dataset/datasetX.csv` containing few customer information and is labeled (i.e., the target column, `Exited` is present). Here below is a brief summary of the features you will find in the datasets:\n",
    "\n",
    "* `CustomerId`: a unique identifier for each customer within the dataset. These values are not ordered sequentially within the dataset, and are only used to identify a specific customer. It typically does not have any influence to whether a customer leaves the business.\n",
    "* `Surname`: A string used to identify the customer in the dataset. Surname may be distinct amidst all or most customers. Because of this, it most likely won't affect the target variable. \n",
    "* `CreditScore`: a numeric representation of the customer's individual fiscal credit score. Typically used to indicate eligibility for loans. Current credit scores use a range from 300 to 850, but the FICO auto score range uses 250-900. This feature likely determines retention rate of customers. \n",
    "* `Geography`: this feature contains a categorical string representing the name of a country the customer is from originally. \n",
    "* `Gender`: this feature contains a categorical string representing the gender of the customer (\"Male\"/\"Female\"). \n",
    "* `Age`: a numerical integer representation of a customer's age. Intuition suggests that older customers are likely to have higher retention than younger customers.\n",
    "* `Tenure`: a numerical integer representation. It is assumed that this feature represents the number of total years the customer has been retained. It is likely that customers which have been retained longer will continue to be retained.\n",
    "* `Balance`: a numerical floating point number (to two decimal places of precision) indicating the customer's current bank balance (assumed total across all accounts). Customers with a greater balance may be less likely to exit the account due to difficulty of transfer. \n",
    "* `NumOfProducts`: numeric integer value. It is assumed that this value represents the number of accounts (products) that this customer has open. Further evaluation of this feature would be needed to determine the usefulness of this feature, but at face-value, intuition dictates that a customer with more products is less likely to exit. \n",
    "* `HasCrCard`: boolean flag (0 or 1) representing whether the customer has a credit card or not. \n",
    "* `IsActiveMember`: boolean flag (0 or 1) representing whether the customer is an active member of the bank. It is assumed this indicates whether the customer has transactions on the regular banking statement. Intuition dictates that inactive members are more likely to exit. \n",
    "* `EstimatedSalary`: numerical floating point representation of the customer's predicted salary (to two decomal places) intuition dictates that customers with different incomes may behave differently with respect to retention rate. \n",
    "* `Exited`: boolean flag (0 or 1) representing whether the customer has exited their account. This is the target variable for the dataset. It should not be dropped, but should not be included as the training input (X), and should instead be separated as the target label (y). \n",
    "\n",
    "You will also see an empty directory `saved_models/`, that is for you to save all the models you'd train in this assignment.\n",
    "\n",
    "`figures/` directory contains few image files used to properly document this assignment. Please do not delete and when possible please move them with this jupyter notebook for proper display of its contents.\n",
    "\n",
    "> In this Jupyter notebook please write your solutions / codes in the cells marked with `#Your solution goes here...`. You may add additional code cells after that cell if you desire. But, please do not remove any cell originally given in the notebook.\n",
    "\n",
    "> After you solve the assignment in the jupyter notebook, be sure to execute and save it so that execution/results/printouts are also saved with it.\n",
    "> Finally, submit only the saved jupyter notebook (`2024-Spring-DL-PA1-assignment.ipynb`) in Canvas to receive grade. For this assignment, Canvas only will accept the jupyter notebook in \"*.ipynb\" extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : (10 points)\n",
    "* Define a function named `summarize_dataset` that takes only one argument: `csv_file`, where `csv_file` is the name of the given `csv` file with this assignment, i.e., `datasetX.csv`. \n",
    "  * The function is expected to summarize the given dataset in the following way:\n",
    "```\n",
    "total number of rows = a\n",
    "total number of columns = b\n",
    "number of columns having non-numeric values = c\n",
    "columns with missing values = [ (d1, e1)  (d2, e2), ... ]\n",
    "gender based summary of exited column = [ (f1, g1)  (f2, g2), ... ]\n",
    "age based summary of exited column = [ ('below or equal to 40', h1)  ('above 40', h2) ]\n",
    "credit score summary =  i +/- j \n",
    "```\n",
    "  \n",
    "where,\n",
    "\n",
    "* `a` is total number of rows in the dataset.\n",
    "* `b` is total number of columns in the dataset.\n",
    "* `c` is number of columns having non-numeric values.\n",
    "* $(d_i, e_i)$ (i.e., a pair/tuple entry) represents column name ($d_i$) and number of missing values present in that column ($e_i$). If number of missing values in a column is zero (0), you do not need to list it. Please sort the tuple entries in descending order of $e_i$ values.\n",
    "* $g_i$ represents the percentage of gender $f_i$ who exited. Please sort the tuple entries entries in descending order of $g_i$ values. Also, print the percentages in 2 decimal places after the decimal point, and print use `%` symbol after the percentage value.\n",
    "* $h_1$ and $h_2$ represents the percentage of $\\leq 40$ year olds who exited, and the percentage $>40$ year olds who exited.  Also, print the percentages in 2 decimal places after the decimal point, and print use `%` symbol after the percentage value.\n",
    "* `j` and `k` are average and standard deviation of credit scores among the data samples respectively. Please print the way it is shown above. Also, print the both values in 2 decimal places after the decimal point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution goes here...\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def summarize_dataset(csv_file):\n",
    "    \n",
    "    \"\"\"\n",
    "    Summarize a churn dataset for the European Bank EBQ from a CSV file.\n",
    "    \n",
    "    This function processes a CSV file containing churn data of the European Bank EBQ,\n",
    "    summarizing important statistics to aid in predictive analysis for customer retention.\n",
    "    It outputs the total number of rows and columns, identifies non-numeric and missing values,\n",
    "    and provides detailed summaries based on gender, age, and credit scores.\n",
    "    \n",
    "    This summary includes statistical insights crucial for preparing the data for an Artificial\n",
    "    Neural Network (ANN) model aimed at predicting customer churn.\n",
    "\n",
    "    The function provides the following summary statistics:\n",
    "    - Total number of rows and columns in the dataset.\n",
    "    - Number of columns containing non-numeric values.\n",
    "    - A list of columns with missing values alongside the count of missing entries for each.\n",
    "    - A gender-based summary of the 'Exited' column, indicating the percentage of customers \n",
    "      from each gender who have exited.\n",
    "    - An age-based summary of the 'Exited' column, categorizing customer exits by age groups: \n",
    "      40 and below, and above 40.\n",
    "    - A summary of the 'CreditScore' column, including its mean and standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_file (str): The filepath of the CSV file to be analyzed. This file should be \n",
    "                      formatted correctly and include at least the following columns: \n",
    "                      'Gender', 'Age', 'Exited', and 'CreditScore'. \n",
    "\n",
    "    Outputs:\n",
    "    - Total number of rows and columns in the dataset.\n",
    "    - Number of columns having non-numeric values.\n",
    "    - Columns with missing values, sorted in descending order by the number of missing values.\n",
    "    - Gender-based summary of the 'Exited' column, showing the percentage of customers who exited,\n",
    "      sorted in descending order by percentage.\n",
    "    - Age-based summary of the 'Exited' column, categorizing customers as 'below or equal to 40'\n",
    "      and 'above 40', and displaying the exit percentages for these categories.\n",
    "    - Credit score summary, including the mean and standard deviation of credit scores among the customers.\n",
    "    \n",
    "    This function prints the summary statistics directly to the console. The information \n",
    "    is organized and displayed in a user-friendly manner, allowing for easy interpretation.\n",
    "\n",
    "    Example Usage:\n",
    "    --------------\n",
    "    >>> summarize_dataset('path/to/datasetX.csv')\n",
    "\n",
    "    Note:\n",
    "    - This function is designed with the assumption that the dataset structure aligns with \n",
    "      the specified requirements. If the dataset's structure or column names differ, \n",
    "      modifications to the function may be necessary.\n",
    "    - Ensure that the dataset does not contain sensitive information or that appropriate \n",
    "      data handling measures are in place to protect privacy.\n",
    "\n",
    "    Returns:\n",
    "    None. The function directly prints the summary statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Determine the total number of rows and columns\n",
    "    total_rows = data.shape[0]\n",
    "    total_columns = data.shape[1]\n",
    "    \n",
    "    # Identify non-numeric columns\n",
    "    non_numeric_columns = data.select_dtypes(include=['object']).shape[1]\n",
    "    \n",
    "    # Identify columns with missing values and their counts\n",
    "    missing_values = data.isnull().sum()\n",
    "    columns_with_missing_values = [(column, missing) for column, missing in missing_values.items() if missing > 0]\n",
    "    columns_with_missing_values.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Gender-based summary of the 'Exited' column\n",
    "    gender_summary = data.groupby('Gender')['Exited'].mean() * 100\n",
    "    gender_summary = [(gender, f\"{percentage:.2f}%\") for gender, percentage in gender_summary.items()]\n",
    "    gender_summary.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Age-based summary of the 'Exited' column\n",
    "    age_below_or_equal_40 = data[data['Age'] <= 40]['Exited'].mean() * 100\n",
    "    age_above_40 = data[data['Age'] > 40]['Exited'].mean() * 100\n",
    "    age_based_summary = [\n",
    "        ('below or equal to 40', f\"{age_below_or_equal_40:.2f}%\"),\n",
    "        ('above 40', f\"{age_above_40:.2f}%\")\n",
    "    ]\n",
    "    \n",
    "    # Credit score summary (mean and standard deviation)\n",
    "    credit_score_mean = data['CreditScore'].mean()\n",
    "    credit_score_std = data['CreditScore'].std()\n",
    "    credit_score_summary = f\"{credit_score_mean:.2f} +/- {credit_score_std:.2f}\"\n",
    "    \n",
    "    # Print the summaries to the console\n",
    "    print(f\"Total number of rows = {total_rows}\")\n",
    "    print(f\"Total number of columns = {total_columns}\")\n",
    "    print(f\"Number of columns having non-numeric values = {non_numeric_columns}\")\n",
    "    print(f\"Columns with missing values = {columns_with_missing_values}\")\n",
    "    print(f\"Gender based summary of exited column = {gender_summary}\")\n",
    "    print(f\"Age based summary of exited column = {age_based_summary}\")\n",
    "    print(f\"Credit score summary = {credit_score_summary}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset via summarize_dataset\n",
    "# Verify accuracy of the path\n",
    "import os\n",
    "\n",
    "# Use the absolute path to the file\n",
    "file_path = \"./dataset/datasetX.csv\"\n",
    "data = summarize_dataset(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that two variables, Age and CreditScore have missingness which will required imputation prior to use in any of the models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "* Preprocessing the given dataset for the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 (10 points)\n",
    "\n",
    "* First preprocessing that we are going to do on the dataset is dropping two features (i.e., columns) that, I think, are irrelevant and would not make any meaningful relationship with the `Exited` feature. The features are: `CustomerId` and `Surname`.\n",
    "* Make sure to create a variable called `dataset_dropped` that will store the revised dataset.\n",
    "* Please print the name of the columns of the revised dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-247a930d79bde8e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Your solution goes here...\n",
    "\n",
    "# Load the dataset where 'data' is our DataFrame loaded via summarize_dataset function  \n",
    "\n",
    "# Drop the 'CustomerId' and 'Surname' columns\n",
    "dataset_dropped = data.drop(['CustomerId', 'Surname'], axis=1)\n",
    "\n",
    "# Store the names of the columns in the revised dataset in a variable\n",
    "dataset_dropped_columns = dataset_dropped.columns\n",
    "\n",
    "# Print the names of the columns in the revised dataset\n",
    "print(\"Revised dataset columns:\", dataset_dropped_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 (10 points)\n",
    "* Second Preprocessing that we are going to do is *Shuffle Rows* of the dataset obtained from `Task 2.1`.\n",
    "* \"It is extremely important to shuffle the training data, so that you do not obtain entire minibatches of highly correlated examples. As long as the data has been shuffled, everything should work OK. Different random orderings will perform slightly differently from each other but this will be a small factor that does not matter much.\" -- [Ian Goodfellow](https://qr.ae/pGBgw8)\n",
    "* Use a random seed value `4321` in case you will call any stochastic method.\n",
    "* Make sure to create a variable called `dataset_shuffled` that will store the revised dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code finds the maximum value in the 'CreditScore' column\n",
    "max_credit_score = dataset_dropped['CreditScore'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of key parts of the next code for dataset_shuffled\n",
    "\n",
    "* sample(frac=1): This shuffles the entire dataset. The frac parameter specifies the fraction of rows to return in the random sample, so frac=1 means return all rows, but in a random order.\n",
    "\n",
    "* random_state=4321: This sets the seed for the random number generator, ensuring that the shuffle operation is reproducible. Using the same seed value means you'll get the same order of shuffled rows every time you run this code.\n",
    "\n",
    "* reset_index(drop=True): After shuffling, the index of the DataFrame will be in the order of the shuffled rows. reset_index(drop=True) resets the index to the default integer index (0, 1, 2, ...) and avoids adding the old index as a column in the DataFrame.\n",
    "\n",
    "This approach ensures that our dataset is shuffled, addressing the concern of avoiding minibatches of highly correlated examples during training, which is crucial for training machine learning models effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "shuff_dataset_final",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Your solution goes here...\n",
    "\n",
    "# Assuming dataset_dropped is our DataFrame obtained from the previous steps\n",
    "# (e.g., after dropping the 'CustomerId' and 'Surname' columns)\n",
    "\n",
    "# Shuffle the rows of the dataset\n",
    "dataset_shuffled = dataset_dropped.sample(frac=1, random_state=4321).reset_index(drop=True)\n",
    "\n",
    "# Print the first few rows of the shuffled dataset to verify\n",
    "print(dataset_shuffled.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: (10 points)\n",
    "\n",
    "* Third Preprocessing that we will do is X-y Partitioning of the dataset obtained from `Task 2.2`.\n",
    "* In its current state, the dataset contains both independent (input, `X`) and the target (output, `y`) features within the same dataframe. For ease of of the training process, we need to partition the training features from the target feature into two separate dataframes. \n",
    "* Make sure, the following cell contains at least two variables: `X` and `y`:\n",
    "  * `X` contains part of the dataset with only independent features, and \n",
    "  * `y` having only the dependent/target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution goes here...\n",
    "\n",
    "# Assuming dataset_shuffled is our DataFrame obtained from the previous step, Task 2.2\n",
    "# and is already shuffled as per the last step\n",
    "\n",
    "# Partition the dataset into X and y\n",
    "# X contains all columns except the target feature 'Exited'\n",
    "X = dataset_shuffled.drop('Exited', axis=1)\n",
    "\n",
    "# y contains only the target feature 'Exited'\n",
    "y = dataset_shuffled['Exited']\n",
    "\n",
    "# Comments:\n",
    "# -------------------\n",
    "# - The .drop('Exited', axis=1) method is used on the shuffled DataFrame to create X.\n",
    "#   This removes the 'Exited' column (specifying axis=1 for columns) and retains all other columns.\n",
    "#   The resulting DataFrame, X, consists of only the independent features.\n",
    "\n",
    "# - To create y, we simply select the 'Exited' column from the dataset_shuffled DataFrame.\n",
    "#   The resulting y holds the values of the dependent (target) variable.\n",
    "\n",
    "#  Print the shapes of X and y to verify the partitioning:\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# This provides a quick check to ensure X contains all features except target, and y contains only target feature. \n",
    "# Step is included to confirm that dataset has been correctly partitioned into independent & dependent variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4 (10 points)\n",
    "* Fourth Preprocessing that we will do is Train-Test Split of X, y obtained from `Task 2.3`.\n",
    "* Now that we have X and y tables with appropriate feature pruning performed, we must split the data into a training partition (`X_train, y_train`) and a testing partition (`X_test, y_test`). \n",
    "* The training partitions (`X_train, y_train`) will be used to train your model, while the test partition (`X_test, y_test`) will be set aside during the training steps, and will only be used to evaluate the trained model. \n",
    "* Training and test splits should be mutually exclusive to the datasets... i.e., a sample can not be both in training and test sets.\n",
    "* Please perform a 80-20 split, meaning 80% of the (X,y) dataset will be in (X_train, y_train) split, while, remaining 20% will be in (X_test,y_test) split. \n",
    "* Please use random seed `4321` prior to calling any stochastic methods.\n",
    "* Make sure the following cell contains at least 4 variables: `X_train`, `y_train`, `X_test`, `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "train_test_split",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Your solution goes here...\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X and y have been defined in previous steps as the features and target variable, respectively\n",
    "\n",
    "# Perform an 80-20 train-test split\n",
    "# Use random_state=4321 to ensure reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4321)\n",
    "\n",
    "#  Comments:\n",
    "# -------------------\n",
    "# - `train_test_split` is called with X & y, datasets containing independent features and target variable.\n",
    "# - `test_size=0.2` specifies that 20% of data should be allocated to test set, & 80% will be used for training.\n",
    "# - `random_state=4321` sets seed for the random number generator used to shuffle data before splitting. \n",
    "# - This ensures that the split is reproducible; running this code multiple times will produce the same split, which is important for experimental repeatability.\n",
    "\n",
    "# Print the shapes of the train and test sets to verify the split:\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "# This will confirm that the dataset has been split according to the specified proportions.\n",
    "# The shapes of X_train and y_train should match, and the shapes of X_test and y_test should match.\n",
    "# This ensures that each feature vector in X has a corresponding target variable in y.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5 (10 points)\n",
    "\n",
    "* Fifth preprocessing that we will do is the *Conversion of Categorical features to Numerical*\n",
    "* Please adopt the `One Hot Encoding` method instead of `Label Encoding` while converting the categorical features. \n",
    "* Make sure the following cell contains a variable named `X_train_ohe` that would contain one hot encoded `X_train` data; on the two categorical columns: 'Geography','Gender'. Please save the encoder for later use; e.g., encode `X_test` dataset, or any future test sample given to you. Under any circumstance, you must not encode `X_test` independently like you would do for `X_train`.\n",
    "* Now, encode the `X_test` data using the one hot encoder you saved while you encoded the `X_train`, and name the variable `X_test_ohe`.\n",
    "\n",
    "\n",
    "* **Both encoding techniques are outlined below**:\n",
    "> A little background first: Categorical features are features that contain values that are not numeric. It would be absurd to work with non-numeric features if you ask neurons in your ANN to compute the weighted sum of inputs, and then pass through activation function, right? These maths are undefined. An obvious solution you may be intrigued to do is dropping the features! Aha! Wrong!! Every piece of data is precious... may present with valuable insights of the data samples to find the patterns to map inputs with output/targets. So, we should include them. But, how?\n",
    "\n",
    "The answer is via \"Encoding\". \n",
    "\n",
    "Several types of encodings are used in practice. Here below are just 2 popular ones:\n",
    "1. **Label Encoding**, where labels are encoded as subsequent numbers. Say, for a categorical feature named \"Category\" with three categorical values: {“Cat”, “Dog” or “Zebra”} can be encoded to \"0\", \"1\", \"2\" respectively as in figure below. The issue with this type of encoding may unintentionally impose a type of ordering of the categories, that may add bias to the training.\n",
    "\n",
    "\n",
    "![label-encoding](figures/le.png)\n",
    "\n",
    "2. **One Hot Encoding**, ignores the ordering of the categories all together. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1. Also, don't forget to remove the original categorical features. Here below just an example, how to convert the categorical feature called \"Category\" having the {“Cat”, “Dog” or “Zebra”} values into three new binary features: \"Cat\", \"Dog\", \"Zebra\".\n",
    "\n",
    "![label-encoding](figures/ohe.png)\n",
    "\n",
    "**A note on the Dummy Variable Trap**\n",
    "The Dummy Variable Trap occurs when two or more dummy variables created by one-hot encoding are highly correlated (i.e., becomes multi-collinear). This means that one variable can be predicted from the others, making it difficult to interpret predicted coefficient variables in regression models. In other words, the individual effect of the dummy variables on the prediction model can not be interpreted well because of multicollinearity.\n",
    "\n",
    "Using the one-hot encoding method, a new dummy variable is created for each categorical variable to represent the presence (1) or absence (0) of the categorical variable. For example, if tree species is a categorical variable made up of the values pine, or oak, then tree species can be represented as a dummy variable by converting each variable to a one-hot vector. This means that a separate column is obtained for each category, where the first column represents if the tree is pine and the second column represents if the tree is oak. Each column will contain a 0 or 1 if the tree in question is of the column's species. These two columns are multi-collinear since if a tree is pine, then we know it's not oak and vice versa. The machine learning models trained on dataset having this multi-collinearity suffers. A remedy is to drop first (or any one) of the dummy (i.e., one-hot) features created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below, we set up a ColumnTransformer object from the sklearn.compose module of the scikit-learn library; The column_transformer is called a ColumnTransformer object, specifically from the scikit-learn library. It is a utility for performing column-wise transformations within a machine learning pipeline, allowing different columns of the input data to undergo distinct preprocessing steps before further analysis or model training\n",
    "\n",
    "- ColumnTransformer is a utility that allows different columns of the input data matrix to be transformed differently. This is useful in datasets where different types of features (e.g., categorical, numerical) require different preprocessing steps.  This class is designed to apply different transformations to different columns of a dataframe or NumPy array. \n",
    "\n",
    "- `transformers`: list of transformations to apply. Each element in the list is a tuple containing three elements:\n",
    "    - First element of the tuple ('encoder') is a name or identifier for the transformer.\n",
    "    - Second element (OneHotEncoder(drop='first')) specifies the transformer object to apply. OneHotEncoder is specified with parameter drop='first', which means for each categorical column, the first category is dropped to avoid creating collinear features (to avoid the dummy variable trap).\n",
    "    - Third element of the tuple (['Geography', 'Gender']) specifies the columns to which the transformer should be applied. The square brackets denote a list in Python, meaning that the OneHotEncoder is applied to both the 'Geography' and 'Gender' columns.\n",
    "- remainder='passthrough': Parameter that specifies what to do with remaining columns not explicitly selected for transformation in transformers list. By setting remainder='passthrough', all columns not specified in the transformers list are passed through without changes and are concatenated to the output of the transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the transformations described above, we will also check missingness for each variable amd impute based on percent missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the percentage of missing values in each column\n",
    "def calculate_nan_percentage(df):\n",
    "    nan_percentages = df.isnull().mean() * 100\n",
    "    return nan_percentages\n",
    "\n",
    "# Calculate the percentage of missing values for each column in the dataset\n",
    "missing_percentages = calculate_nan_percentage(X_train)\n",
    "\n",
    "missing_percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the percentages of missing values in age and CreditScore are 4.41% and 0.29% respectively.\n",
    "\n",
    "- Under 1%: Missingness is often inconsequential, and any form of imputation (mean, median, mode) can be used.\n",
    "- 1% - 5%: Simple imputations like mean/median for numerical features, or mode for categorical features, are typically safe.\n",
    "- 5% - 15%: More caution is needed. Consider using model-based methods, like K-Nearest Neighbors (KNN) or Multiple Imputation by Chained Equations (MICE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with missing values and get their names\n",
    "columns_with_missing_values = X_train.columns[X_train.isnull().any()]\n",
    "\n",
    "# Print the column names with missing values\n",
    "print(\"Columns with missing values:\", columns_with_missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "one-hot-encoding-function",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Your solution goes here...\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# Assuming X_train and X_test are defined and include 'Geography' and 'Gender'\n",
    "\n",
    "# Define the column transformer with OneHotEncoder for 'Geography' and 'Gender'\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('encoder', OneHotEncoder(drop='first'), ['Geography', 'Gender'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Fit and transform the X_train dataset, then transform the X_test dataset\n",
    "X_train_ohe = column_transformer.fit_transform(X_train)\n",
    "X_test_ohe = column_transformer.transform(X_test)\n",
    "\n",
    "# Retrieve column names for the one-hot encoded features\n",
    "ohe_feature_names = column_transformer.named_transformers_['encoder'].get_feature_names_out(input_features=['Geography', 'Gender'])\n",
    "\n",
    "# Identify non-transformed (passed through) columns\n",
    "non_transformed_cols = [col for col in X_train.columns if col not in ['Geography', 'Gender']]\n",
    "\n",
    "# Combine all column names\n",
    "all_feature_names = np.concatenate((ohe_feature_names, non_transformed_cols))\n",
    "\n",
    "# Convert the output arrays to DataFrames with the proper column names\n",
    "X_train_ohe = pd.DataFrame(X_train_ohe, columns=all_feature_names)\n",
    "X_test_ohe = pd.DataFrame(X_test_ohe, columns=all_feature_names)\n",
    "\n",
    "# Initialize imputers\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Apply mean imputation to 'CreditScore'\n",
    "X_train_ohe['CreditScore'] = mean_imputer.fit_transform(X_train_ohe[['CreditScore']])\n",
    "\n",
    "# Apply KNN imputation to 'Age'\n",
    "X_train_ohe['Age'] = knn_imputer.fit_transform(X_train_ohe[['Age']])\n",
    "\n",
    "\n",
    "#  Comments:\n",
    "# -------------------\n",
    "# - The OneHotEncoder is configured with drop='first' to avoid the dummy variable trap by dropping the first category in each categorical variable. This reduces the number of dummy variables by one per feature, mitigating multicollinearity.\n",
    "# - The ColumnTransformer allows us to specify which columns should be one-hot encoded while leaving the rest of the dataset unchanged. It's a flexible tool for applying different transformations to different columns of a dataset.\n",
    "# - We fit the ColumnTransformer to the X_train dataset using `fit_transform`, which both learns the encoding scheme (i.e., which categories exist in the data) and applies the transformation. \n",
    "# - For the X_test dataset, we use `transform` without fitting, ensuring that the same encoding scheme learned from X_train is applied. This is crucial to maintain consistency in the feature space between training and testing datasets.\n",
    "# - It's important not to fit the encoder to X_test because doing so would allow the encoder to learn potentially different categories, leading to inconsistencies between the encoded features of the training and testing sets. \n",
    "# - Using the encoder fitted on the training set ensures that the testing set is transformed according to the same scheme, even if some categories are missing in the testing set.\n",
    "\n",
    "# Note: After transforming with OneHotEncoder, the output is a NumPy array. \n",
    "# If we need to work with pandas DataFrame (i.e., to keep column names), we will need to convert it back to DataFrame. This might also involve manually setting the column names to reflect the one-hot encoded features and any other features passed through.\n",
    "# Approach to convert output of ColumnTransformer back to a pandas DataFrame & manually handle column names for \n",
    "# one-hot encoded features along with other features is detailed in the next code chunk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To convert the output of the ColumnTransformer back to a pandas DataFrame and manually handle column names for one-hot encoded features along with other features, we can follow these steps:\n",
    "* .named_transformers_ is an attribute of the ColumnTransformer object and is a dictionary that stores the individual transformers used within the ColumnTransformer, accessible by their names. When we define a ColumnTransformer, we can assign a name to each transformer operation, which allows us to later reference these transformers by their given names.\n",
    "\n",
    "* Retrieve Column Names for One-Hot Encoded Features: Use the `get_feature_names_out` method from the OneHotEncoder to get the names of the one-hot encoded features.\n",
    "\n",
    "* After applying transformations like OHE, the original feature names often change. A single categorical feature with three categories would be transformed into three separate binary features. The `get_feature_names_out()` function generates the names for these new features based on the categories found in the data.\n",
    "\n",
    "* Identify Non-Transformed Columns: Determine which columns were not transformed (i.e., passed through) and maintain their original names.\n",
    "\n",
    "* Combine All Column Names: Create a comprehensive list of column names that includes both the one-hot encoded feature names and the names of the non-transformed features.\n",
    "\n",
    "* Convert the Output Arrays to DataFrames: Use the comprehensive list of column names when converting the NumPy arrays (results of transformations) back into pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The ColumnTransformer 'column_transformer' has already been defined and fitted as previously shown\n",
    "\n",
    "# Step 1: Retrieve column names for the one-hot encoded features\n",
    "ohe_feature_names = column_transformer.named_transformers_['encoder'].get_feature_names_out()\n",
    "\n",
    "# Step 2: Identify non-transformed (passed through) columns\n",
    "# ColumnTransformer does not directly provide the names of the untouched columns.\n",
    "# Thus we will need to extract from column names that are not 'Geography' or 'Gender'- not encoded, e.g., from X_train\n",
    "non_transformed_cols = [col for col in X_train.columns if col not in ['Geography', 'Gender']]\n",
    "\n",
    "# Step 3: Combine all column names (ensure the order matches the order of columns in the transformed array)\n",
    "all_feature_names = list(ohe_feature_names) + non_transformed_cols\n",
    "\n",
    "# Step 4: Convert the output arrays to DataFrames with the proper column names\n",
    "X_train_ohe_df = pd.DataFrame(X_train_ohe, columns=all_feature_names)\n",
    "X_test_ohe_df = pd.DataFrame(X_test_ohe, columns=all_feature_names)\n",
    "\n",
    "# Note: The DataFrame conversion assumes that the order of columns in the transformed array matches the order\n",
    "# in 'all_feature_names'. If we apply other transformations or if order does not match, we will need to adjust accordingly.\n",
    "\n",
    "# Now, X_train_ohe_df and X_test_ohe_df are pandas DataFrames with columns properly named.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6: (10 points)\n",
    "\n",
    "* Sixth Preprocessing that we are going to do is *Normalization of X_train_ohe, and X_test_ohe*\n",
    "\n",
    "* Now that we have all numerical training and test datasets: `X_train_ohe` and `X_test_ohe` respectively, we can normalize each features in both of the datasets. **Normalization** is just one of the way to scale each feature. In class you'll learn a ton of other ways to scale. For this task, let's resort to **Normalization**.\n",
    "\n",
    "> \"The rule of thumb for scaling datasets, is we scale training dataset first, then using the statistics that we learn during the scaling process, we scale the test dataset. We do not learn any new statistics while we scale the test dataset.\"\n",
    "\n",
    "* Also, scaling is commonly performed column-wise, and never sample/row wise.\n",
    "\n",
    "* Make sure the following cell contains the two scaled variables: `X_train_scaled` and `X_test_scaled` based on the requirements mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the normalization that will be performed in the next code chunk:\n",
    "\n",
    "* Maintaining Data Integrity: By fitting the scaler only on the training data, we ensure that the model is not exposed to any information from the test dataset during training. This practice prevents information leakage and ensures that the model's performance metrics are a reliable indicator of how well the model will perform on unseen data.\n",
    "\n",
    "* Applying the Same Scale: Transforming the test data using the scaler fitted on the training data ensures that the test data is scaled using the same parameters (min and max values for each feature) as the training data. This consistency is crucial for the model's ability to generalize from training to testing data.\n",
    "\n",
    "* Column-wise Scaling: Normalization (and scaling in general) is applied column-wise because we want to adjust each feature's values to a specific range while maintaining their distribution across all samples. This process does not mix information between features but standardizes their scales.\n",
    "\n",
    "Now, X_train_scaled and X_test_scaled will be our normalized training and test datasets, respectively, ready for training machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "normalizer-learning-training",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Your solution goes here...\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# X_train_ohe and X_test_ohe are already defined as DataFrames from previous steps\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train_ohe)\n",
    "# transform the test data using the same scaler but without fitting it again\n",
    "X_test_scaled = scaler.transform(X_test_ohe)\n",
    "\n",
    "# Convert the scaled arrays back to DataFrames with the correct column names\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=all_feature_names)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=all_feature_names)\n",
    "\n",
    "\n",
    "# Comments:\n",
    "# -------------------\n",
    "# \"The rule of thumb for scaling datasets is to scale the training dataset first, then use the\n",
    "# statistics (i.e., min and max values for MinMaxScaler) learned during the scaling process to scale\n",
    "# the test dataset. We do not learn any new statistics while we scale the test dataset.\"\n",
    "# This approach ensures that the model is not inadvertently exposed to any information from the test set\n",
    "# during training, maintaining the integrity of the testing process and preventing data leakage.\n",
    "\n",
    "# Scaling is performed column-wise to ensure that each feature is normalized independently.\n",
    "# This is critical because different features may have different scales and distributions,\n",
    "# and normalization allows each feature to contribute equally to the distance calculations in many algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: (10 points)\n",
    "* *Designing your first Artificial Neural Network (ANN) based classifier* using i) **Micrograd**, and ii) **Tensorflow** or PyTorch**.:\n",
    "\n",
    "  > **Micrograd** by Andrej Karpathy [[video](https://youtu.be/VMj-3S1tku0?si=D5m1IJW5AkJzhvLE)][[git-prepo](https://github.com/karpathy/micrograd.git)]\n",
    "\n",
    "  > **Keras/Tensorflow** @ Python reference:  please take a look here [https://keras.io/getting-started/sequential-model-guide/](https://keras.io/getting-started/sequential-model-guide/). \n",
    "\n",
    "  > **PyTorch** @Python reference: Please take a look at [Deep Learning with PyTorch Guide](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "  \n",
    "  \n",
    "### Step 1: The ANN architecture\n",
    "* Let's design the first artificial network architecture for the classifier we would like to build. Here below is one. How did I get this architecture? Maybe in my dream! Haha. Someday you will get one too. Until that, let's follow the architecture below:\n",
    "  ![Task 3 ANN architecture](figures/nn-1.png)\n",
    "  * **Input layer** will have 11 units as the dimension of training set: `X_train_scaled` (i.e, number of columns = 11).\n",
    "  * **First hidden layer** will have 5 neurons, each with \"Rectified Linear Unit (`ReLU``)\" as activation function.\n",
    "  * **Second hidden layer** will have 4 neurons, each with \"`ReLU`\" as activation function.\n",
    "  * **Output layer** will have just 1 neuron, with `sigmoid`` activation function. \n",
    "    * The reason behind a single neuron with `sigmoid` activation at the output layer is that, output of this neuron will tell the probability score of the target outcome: \"Exited\" True or False. If the output neuron produces value above 0.5, we will say the neural network predicted \"True\", otherwise, False. This is the beauty of using sigmoid function at the output layer as we can interpret the output value of the neuron as probability score.\n",
    "* The architecture will come to life when you initiate the training process with training data.\n",
    "  * The training process needs a g**radient descend based optimizer**, and a convex looking **loss function**.  \n",
    "  * For this task, let's choose the `adam` optimizer, and the `binary_crossentropy` as the loss function.\n",
    "### Step 2: The Training process\n",
    "* Let's start the training process with the training dataset, `X_train_scaled`.\n",
    "  * Gradient descend based optimization updates run in iterations. When number of iterations equal the total number of training samples, we call that `1 epoch` has passed. Let's continue the training for `25 epochs`. But, you are welcome to run longer than this. There are, however, simpler way to determine if you should early stop your training. \n",
    "    * (Optional) Can you extract information about optimization in each epoch? If so, draw a epoch-loss plot, where X-axis needs to show epoch numbers, and Y-axis will show the `binary_crossentropy` loss value in that particular epoch iteration.\n",
    "* Don't forget to save the model into a file in the `saved_models/` directory so that you can re-use it later for further prediction. Let's give it a name: `model-ann-11-5-4-1-xx` with an extension of your choosing, with `xx` must be replaced by any of `{mc,pt,tf}`, where `mc` to denote if that's a micrograd based model that you are saving, or `pt` for a pytorch model, or `tf` for a tensorflow model.\n",
    "\n",
    "### Step 3: The Evaluation\n",
    "\n",
    "#### (part 3.1) Evaluating your model with the entire test dataset:\n",
    "\n",
    "* Load your trained model `model-ann-11-5-4-1-xx` from the file, and have it predict the entire test set you have at head: (`X_test_scaled`). Luckily, for each of the test sample in the set, you also have ground true `Exited` value in the `y_test`. \n",
    "* Please report/print your model's predictive performance on the test set in terms of `accuracy`, `precision`, `recall`, and `F1 scores`.\n",
    "\n",
    "#### (part 3.2) Evaluating your model with 1 test sample with known Exited value\n",
    "\n",
    "* Here is a single test sample for which we know the ground true `Exited` value:\n",
    "\n",
    "| CustomerId | Surname | CreditScore | Geography | Gender | Age | Tenure | Balance | NumOfProducts | HasCrCard | IsActiveMember |EstimatedSalary | Exited |\n",
    "| :---        |    :----:   |          ---: | :---        |    :----:   |          ---: | :---        |    :----:   |          ---: | :---        |    :----:   |          ---: |          ---: |\n",
    "| 55443322 | Reynolds |709|Germany|Male|30|9|115479.48|2|1|1|134732.99|0|\n",
    "\n",
    "* Load your trained model `model-ann-11-5-4-1-xx` from the file, and have it predict the test sample above. Please don't forget to preprocess this test samples so that it is compliant with the input and model requirements.\n",
    "* Please report whether it predicts a 0 or 1 for the `Exited` target, and also comment whether your model makes a mistake or predicts correctly.\n",
    "\n",
    "#### (part 3.3) Evaluating your model with 1 test sample without known Exited value\n",
    "\n",
    "* Here is a single test sample for which we **do not know** the ground true `Exited` value:\n",
    "\n",
    "| CustomerId | Surname | CreditScore | Geography | Gender | Age | Tenure | Balance | NumOfProducts | HasCrCard | IsActiveMember |EstimatedSalary | \n",
    "| :---        |    :----:   |          ---: | :---        |    :----:   |          ---: | :---        |    :----:   |          ---: | :---        |    :----:   |          ---: |\n",
    "| 55443323 | Nguyen |603|France|Female|76|20|123456.78|5|1|1|55000.00|\n",
    "\n",
    "\n",
    "* Load your trained model `model-ann-11-5-4-1-xx` from the file, and have it predict the test sample above. Please don't forget to preprocess this test samples so that it is compliant with the input and model requirements.\n",
    "* Please report whether it predicts a 0 or 1 for the `Exited` target. Can you comment on this data sample whether your model captured the pattern in the population?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rationale for the proposed ANN is as follows:\n",
    "\n",
    "Input Layer: The 11 units corresponds to the number of features (excluding CustomerId, Surname, and Exited) that are directly related to the customer's behavior and attributes which may influence their decision to leave the bank. These attributes provide foundational data required for predicting churn. \n",
    "\n",
    "First Hidden Layer: The first hidden layer with 5 neurons is a design choice that represents a level of abstraction derived from the input data. The use of ReLU (Rectified Linear Unit) activation function is suitable for introducing non-linearity into the model, allowing it to learn complex patterns in the data. The number of neurons (5) is fairly arbitrary but represents a balance between model complexity and computational efficiency. It's enough to begin processing the patterns from inputs without being overly complex.\n",
    "\n",
    "Second Hidden Layer: The second hidden layer with 4 neurons continues the process of abstraction and pattern recognition in the data. Again, employing ReLU ensures the model can capture non-linear relationships. The decrease in neurons (from 5 to 4) from the first to the second hidden layer further refines the representations without making the network too wide. This slight reduction potentially reduces the risk of overfitting on the training data.\n",
    "\n",
    "Output Layer: The output layer consists of a single neuron with a sigmoid activation function, which is a standard approach for binary classification problems like churn prediction (target variable Exited is either 0 or 1). The sigmoid function outputs a probability score between 0 and 1, indicating the likelihood of a customer exiting. A threshold of 0.5 is commonly used to classify outcomes as True (churn) or False (no churn), making the output easily interpretable as a probability.\n",
    "\n",
    "Training Process\n",
    "•\tOptimizer: The Adam optimizer, favored for deep learning model training, merges AdaGrad and RMSProp's strengths, enhancing stochastic gradient descent. It employs exponentially decaying averages of past gradients for momentum and adjusts learning rates using these averages, ensuring efficiency and suitability for large datasets with varying gradient characteristics. This approach ensures computational efficiency and adaptability, particularly for data with noise or sparsity. \n",
    "•\tLoss Function: Binary cross entropy is an appropriate loss function for binary classification problems. It measures the distance between the distribution of the predictions and the true distribution of the target variable. For churn prediction in this data, it quantifies how well the model predicts the actual customer churn status, making it an appropriate choice for optimizing the model during training.\n",
    "\n",
    "Overall, the performance and the choice of hyperparameters such as the number of neurons in the hidden layers may need to be fine-tuned based on the actual data and through validation techniques to achieve optimal results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "print(y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-calculate percentage of missing values in each column to verify that the imputation was successful\n",
    "def calculate_nan_percentage(df):\n",
    "    nan_percentages = df.isnull().mean() * 100\n",
    "    return nan_percentages\n",
    "\n",
    "# Calculate the percentage of missing values for each column in the dataset\n",
    "missing_percentages = calculate_nan_percentage(X_train_scaled)\n",
    "\n",
    "missing_percentages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch approach for part 3.1: Evaluating your model with the entire test dataset:**\n",
    "\n",
    "- First we will Load our trained model model-ann-11-5-4-1-xx from the file, and have it predict the entire\n",
    "test set we have at head: (X_test_scaled). Luckily, for each of the test sample in the set, we also have ground true Exited value in the y_test.\n",
    "\n",
    "- Please report/print your model’s predictive performance on the test set in terms of accuracy,\n",
    "precision, recall, and F1 scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information relevant to the ANN architecture, training, and evaluation\n",
    "\n",
    "- import torch.nn as nn imports the neural network module from PyTorch (torch.nn). This module contains all the building blocks for creating neural networks, such as layers (e.g., linear, convolutional) and activation functions.\n",
    "\n",
    "- import torch.nn.functional as F imports the functional API from PyTorch, which provides functions for operations like activation functions (ReLU, sigmoid, etc.), loss functions, and convolution operations. It's a stateless alternative to the nn module.\n",
    "\n",
    "- import torch.optim as optim imports the optimization module from PyTorch (torch.optim). This module includes various optimization algorithms for training neural networks, such as SGD, Adam, etc.\n",
    "\n",
    "- y_train_tensor = torch.FloatTensor(y_train.values).view(-1, 1) This line converts the y_train data into a PyTorch tensor of type FloatTensor, which is necessary for computations in PyTorch models. The .values extracts the values from a pandas Series or DataFrame as a NumPy array. The view(-1, 1) reshapes the tensor to ensure it has a single column, making it suitable for operations expecting a specific input shape (e.g., the loss function). The -1 in the view function infers the appropriate size for that dimension based on the original size, ensuring the total number of elements remains constant.\n",
    "\n",
    "- ReLU (Rectified Linear Unit) is an Activation Function used as a nonlinear activation function in neural networks. Its primary role is to introduce nonlinearity into the network, allowing the model to learn complex patterns in the data.  The ReLU function is defined as f(x)=max(0,x), meaning it outputs the input directly if it is positive; otherwise, it will output zero.  It is applied to the output of neurons in intermediate layers of the network. For example, after a linear transformation in a layer, ReLU can be applied to each neuron's output before passing it to the next layer.  ReLU helps mitigate the vanishing gradient problem, accelerates the convergence of stochastic gradient descent compared to sigmoid or tanh functions, and is computationally efficient. Limitations: It can lead to dying ReLU problem, where neurons output zero for all inputs and gradients cannot flow through the neuron during backpropagation.\n",
    "\n",
    "- Adam (Adaptive Moment Estimation) is an optimization algorithm used to minimize the loss function during the training of neural networks. Its role is to update the network weights iteratively based on training data to reduce the difference between the predicted output and the actual label.  Adam combines ideas from two other extensions of stochastic gradient descent: AdaGrad, which adapts the learning rate for each parameter, and RMSProp, which uses a moving average of squared gradients to normalize the gradient.\n",
    "\n",
    "It is used during the training phase of the model, specifically for updating the weights after each batch of data is processed.\n",
    "\n",
    "Adam is known for its effectiveness in practice, especially on problems with large datasets or parameters. It automatically adjusts the learning rate during training, is computationally efficient, and has little memory requirements.\n",
    "Limitations: Despite its popularity, Adam might not converge to the optimal solution under certain conditions, and tuning its hyperparameters (learning rate, beta values) is crucial for optimal performance.\n",
    "\n",
    "ReLU and Adam serve complementary purposes in the construction and training of ANNs, with ReLU focusing on the model's internal data processing and Adam on optimizing the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.1 Solution\n",
    "\n",
    "#### The code chunk below addresses the steps defined as: \n",
    "\n",
    "- Step 1: The ANN architecture\n",
    "- Step 2: The Training process\n",
    "- Step 3: The Evaluation: Model evaluation with the entire test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ANN-1_layers",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Your solution using PyTorch implementing the training process with stopping criteria\n",
    "\n",
    "# Here class ChurnModel defines our neural network architecture.\n",
    "\n",
    "# X_train_scaled and y_train are our features and labels respectively\n",
    "# Code to defiine the model using PyTorch\n",
    "# Layers are defined as:\n",
    "# - Input layer will have 11 units as the dimension of training set: X_train_scaled (i.e,\n",
    "# - First hidden layer will have 5 neurons, each with “Rectified Linear Unit (‘ReLU“)” as activation function.\n",
    "# - Second hidden layer will have 4 neurons, each with “ReLU” as activation function.\n",
    "# - Output layer will have just 1 neuron, with ‘sigmoid“ activation function.\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "class ChurnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChurnModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(11, 5)\n",
    "        self.layer2 = nn.Linear(5, 4)\n",
    "        self.output = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = torch.sigmoid(self.output(x))  # Ensure output is between 0 and 1\n",
    "        return x\n",
    "\n",
    "#  Our X_train_scaled and y_train are already defined and the former is scaled properly\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled.values)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = ChurnModel()\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = Adam(model.parameters(), lr=0.001)  # Learning rate can be adjusted\n",
    "\n",
    "\n",
    "epochs = 25\n",
    "loss_values = []\n",
    "early_stopping_threshold = 0.001\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "\n",
    "    # Debugging: Directly check outputs range before loss calculation\n",
    "    print(\"Output range before loss calculation:\", outputs.min().item(), outputs.max().item())\n",
    "    \n",
    "    # Ensure outputs are strictly within the [0, 1] range\n",
    "    outputs = torch.clamp(outputs, 0, 1)\n",
    "\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_values.append(loss.item())\n",
    "    if epoch > 1 and abs(loss_values[-2] - loss_values[-1]) < early_stopping_threshold:\n",
    "        print(\"Early stopping criteria met\")\n",
    "        break\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "# Plotting the loss values\n",
    "plt.plot(range(1, len(loss_values)+1), loss_values, marker='o')\n",
    "plt.title('Epoch vs. Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "model_type = 'pt'  # 'mc' for micrograd, 'pt' for PyTorch, 'tf' for TensorFlow\n",
    "saved_models_dir = 'saved_models'\n",
    "if not os.path.exists(saved_models_dir):\n",
    "    os.makedirs(saved_models_dir)\n",
    "model_filename = f'model-ann-11-5-4-1-{model_type}.pth'\n",
    "model_save_path = os.path.join(saved_models_dir, model_filename)\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Load the model for evaluation\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "model.eval()\n",
    "\n",
    "# Evaluation on the Test set\n",
    "# X_test_scaled and y_test are defined and scaled\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled.values)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).view(-1, 1)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred_class = (y_pred >= 0.5).float()\n",
    "    accuracy = accuracy_score(y_test_tensor.numpy(), y_pred_class.numpy())\n",
    "    precision = precision_score(y_test_tensor.numpy(), y_pred_class.numpy())\n",
    "    recall = recall_score(y_test_tensor.numpy(), y_pred_class.numpy())\n",
    "    f1 = f1_score(y_test_tensor.numpy(), y_pred_class.numpy())\n",
    "\n",
    "print(\"Model Evaluation Results on Test Set:\")\n",
    "print(f\" - The model's Accuracy: {accuracy:.4f} indicates the proportion of the total number of predictions that were correct.\")\n",
    "print(f\" - The model's Precision: {precision:.4f} reflects the proportion of positive identifications that were actually correct. It shows how reliable the model is when it predicts a sample as positive.\")\n",
    "print(f\" - The model's Recall: {recall:.4f} measures the proportion of actual positives that were identified correctly. It highlights the model's capability to find all relevant cases within the dataset.\")\n",
    "print(f\" - The model's F1 Score: {f1:.4f} is the harmonic mean of precision and recall, providing a balance between them. It's particularly useful when the class distribution is uneven.\")\n",
    "\n",
    "# Example: Including the model's architectural summary if needed\n",
    "print(\"\\nModel Architectural Summary:\")\n",
    "print(\" - Input Layer: 11 units (matching the dimensionality of the input features)\")\n",
    "print(\" - First Hidden Layer: 5 neurons with ReLU activation\")\n",
    "print(\" - Second Hidden Layer: 4 neurons with ReLU activation\")\n",
    "print(\" - Output Layer: 1 neuron with Sigmoid activation (for binary classification)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution for (part 3.2) Evaluating your model with 1 test sample with known Exited value\n",
    "\n",
    "* Here is a single test sample for which we know the ground true `Exited` value:\n",
    "\n",
    "| CustomerId | Surname | CreditScore | Geography | Gender | Age | Tenure | Balance | NumOfProducts | HasCrCard | IsActiveMember |EstimatedSalary | Exited |\n",
    "| :---        |    :----:   |          ---: | :---        |    :----:   |          ---: | :---        |    :----:   |          ---: | :---        |    :----:   |          ---: |          ---: |\n",
    "| 55443322 | Reynolds |709|Germany|Male|30|9|115479.48|2|1|1|134732.99|0|\n",
    "\n",
    "* Load your trained model `model-ann-11-5-4-1-xx` from the file, and have it predict the test sample above. Please don't forget to preprocess this test samples so that it is compliant with the input and model requirements.\n",
    "* Please report whether it predicts a 0 or 1 for the `Exited` target, and also comment whether your model makes a mistake or predicts correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both 'column_transformer' and 'scaler' were previously defined and fitted on the training data\n",
    "# Also assuming 'model' is loaded and ready for prediction\n",
    "\n",
    "# Define the single sample data as a dictionary\n",
    "sample_data = {\n",
    "    'CreditScore': [709],\n",
    "    'Geography': ['Germany'],\n",
    "    'Gender': ['Male'],\n",
    "    'Age': [30],\n",
    "    'Tenure': [9],\n",
    "    'Balance': [115479.48],\n",
    "    'NumOfProducts': [2],\n",
    "    'HasCrCard': [1],\n",
    "    'IsActiveMember': [1],\n",
    "    'EstimatedSalary': [134732.99]\n",
    "}\n",
    "\n",
    "# Convert the sample data to a DataFrame\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "\n",
    "# Apply the transformations to the sample\n",
    "# Note: 'fit_transform' is used on training data, but for new data, we use 'transform' only\n",
    "sample_transformed = column_transformer.transform(sample_df)\n",
    "\n",
    "# The 'scaler' function is already fitted to the training data where the scaler = MinMaxScaler\n",
    "sample_scaled = scaler.transform(sample_transformed)\n",
    "\n",
    "# Convert the scaled sample back to a DataFrame (for readability)\n",
    "# Extract the feature names for the transformed columns\n",
    "feature_names = column_transformer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for the transformed and scaled sample\n",
    "sample_scaled_df = pd.DataFrame(sample_scaled, columns=feature_names)\n",
    "\n",
    "# Convert to PyTorch tensor before prediction\n",
    "sample_tensor = torch.FloatTensor(sample_scaled_df.values)\n",
    "\n",
    "### Could this have been done without making the dataframe?  Which is better?\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "# sample_tensor = torch.FloatTensor(sample_scaled)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Predict the 'Exited' value for the sample\n",
    "with torch.no_grad():\n",
    "    prediction = model(sample_tensor)\n",
    "    predicted_class = (prediction >= 0.5).float().item()  # Convert to binary class (0 or 1)\n",
    "\n",
    "# Print the prediction\n",
    "print(f\"Predicted 'Exited' value: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analogous implementation of our ANND with Micrograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach using Micrograd**\n",
    "\n",
    "Micrograd is a minimalistic automatic differentiation library primarily used for educational purposes and developed by Andrej Karpathy. Unlike PyTorch or TensorFlow, Micrograd is much simpler and doesn't come with built-in functionalities for layers, optimizers, or loss functions in a full deep learning framework. Implementing an ANN with Micrograd involves creating these components from scratch.  We will use the approach presented by Karpathy in his Micrograd video.\n",
    "\n",
    "Micrograd is designed to work with its own Value class, which is a tiny scalar autograd engine and does not support tensors or vectorized operations directly. This means that the ANN is implemented with scalar operations.  This is not efficient for real-world applications but serves as a good learning exercise.\n",
    "\n",
    "For implementation with Micrograd , we do not need to implement the `adam` optimizer. Please use the default optimizer found in the code repo.\n",
    "MANDATORY TASK: However, you need to add the `sigmoid` activation capability to follow along with task of the assignment.\n",
    "OPTIONAL TASK: If you are interested, you may implement binary cross-entropy loss function, instead of the loss function developed in the code repo. Once again, this part is optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My Micrograd approach will include:\n",
    "\n",
    "- **Activation Functions**: The sigmoid function is defined for use in the output layer. This matches the activation function used in your PyTorch model.\n",
    "    \n",
    "-**Loss Function**: The binary_cross_entropy function implements the binary cross-entropy loss, suitable for binary classification tasks.\n",
    "   \n",
    "- **Model Components**: The Neuron, Layer, and SimpleANN classes are defined to construct the neural network. Each layer's neurons are initialized with small random weights for diversity in the learning process.\n",
    "    \n",
    "- **Training Loop**: Demonstrates how to iterate over epochs, compute forward passes, calculate loss, perform backpropagation, and update weights manually without an optimizer like Adam.\n",
    "\n",
    "-**Data Conversion:** convert train and test data into a format compatible with Micrograd using Value objects. This preprocessing step is crucial for ensuring that the data is in the correct format for training with Micrograd. By wrapping each number in the Value class, we enable Micrograd to compute gradients with respect to these numbers, which is essential for the optimization step in training neural networks. This conversion should be applied to both the training and testing datasets before using them for model training or evaluation with Micrograd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%pip install micrograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is a comprehensive implementation of a neural network using Micrograd, a minimalist automatic differentiation library. \n",
    "\n",
    "The code encompasses  \n",
    "    - the definition of activation functions, \n",
    "    - a binary cross-entropy loss function, \n",
    "    - the neural network architecture (including neurons, layers, and the overall model), \n",
    "    - data preprocessing, and \n",
    "    - the training loop. \n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- Activation Functions: sigmoid and relu are used for non-linear transformations within the network.\n",
    "\n",
    "- Binary Cross-Entropy Loss: Used for binary classification tasks, provides a measure of the difference between the predicted values and the actual labels.\n",
    "\n",
    "- Neuron and Layer: Building blocks of the network, with neurons forming layers, and layers forming the network.\n",
    "\n",
    "- SimpleANN: Represents the neural network model, consisting of a sequence of layers.\n",
    "\n",
    "- Data Preprocessing: Converts the raw data into a format compatible with Micrograd, ensuring each data point is wrapped in a Value object for automatic differentiation.\n",
    "\n",
    "- Training Loop: Iterates over epochs, shuffles data, creates mini-batches, performs forward and backward passes, and updates model parameters.\n",
    "\n",
    "The model = SimpleANN() line is crucial as it initializes the neural network model that we train with our dataset. This setup forms a complete pipeline for training a neural network using Micrograd, from data preprocessing to model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have devloped my micrograd approach and the full code addressing each of these `Key Points` is below.  \n",
    "\n",
    "I am awaure that testing and improving such a comprehensive code snippet in one go is challenging, especially without the ability to run and verify the output directly in this environment. To effectively test and validate each part of the code, I will break it down into smaller, manageable sections. \n",
    "\n",
    "Here's how I will approach this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    # Sigmoid function for non-linear activation, ensures output [0, 1]\n",
    "    # Use Python's math.exp for the exponential calculation\n",
    "    return 1 / (1 + Value(math.exp(-x.data)))\n",
    "\n",
    "# Define the ReLU activation function\n",
    "def relu(x):\n",
    "    # ReLU (Rectified Linear Unit) for non-linear activation, ensures output is non-negative\n",
    "    return x if x.data > 0 else Value(0)\n",
    "\n",
    "def test_activation_functions():\n",
    "    # Test inputs for activation functions\n",
    "    inputs = [Value(-1), Value(0), Value(1)]\n",
    "\n",
    "    # Expected outputs for sigmoid: approximately sigmoid(-1), sigmoid(0), sigmoid(1)\n",
    "    expected_sigmoid_outputs = [0.26894142, 0.5, 0.73105858]\n",
    "\n",
    "    # Expected outputs for ReLU: relu(-1), relu(0), relu(1)\n",
    "    expected_relu_outputs = [0, 0, 1]\n",
    "\n",
    "    # Test Sigmoid\n",
    "    sigmoid_outputs = [sigmoid(x).data for x in inputs]\n",
    "    assert all(abs(so - eo) < 1e-5 for so, eo in zip(sigmoid_outputs, expected_sigmoid_outputs)), \"Sigmoid function test failed\"\n",
    "\n",
    "    # Test ReLU\n",
    "    relu_outputs = [relu(x).data for x in inputs]\n",
    "    assert all(ro == eo for ro, eo in zip(relu_outputs, expected_relu_outputs)), \"ReLU function test failed\"\n",
    "\n",
    "    print(\"Activation function tests passed.\")\n",
    "\n",
    "# Run the test\n",
    "test_activation_functions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this was a test, you can remove.\n",
    "# Let's simulate a minimal example based on the provided class implementations and generate fake data\n",
    "\n",
    "# Assuming necessary imports and function definitions from the shared code snippets\n",
    "from micrograd.engine import Value\n",
    "\n",
    "# Re-defining the Neuron, Layer, and SimpleANN classes based on the provided implementations\n",
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(0.01 * np.random.randn()) for _ in range(nin)]\n",
    "        self.b = Value(0)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = sum(wi * xi for wi, xi in zip(self.w, x)) + self.b\n",
    "        return act\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout, activation=relu):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [self.activation(neuron(x)) for neuron in self.neurons]\n",
    "        return outs\n",
    "\n",
    "class SimpleANN:\n",
    "    def __init__(self):\n",
    "        self.layer1 = Layer(11, 5, activation=relu)\n",
    "        self.layer2 = Layer(5, 4, activation=relu)\n",
    "        self.output_layer = Layer(4, 1, activation=sigmoid)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x[0]\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in [self.layer1, self.layer2, self.output_layer]:\n",
    "            for neuron in layer.neurons:\n",
    "                params += neuron.w\n",
    "                params.append(neuron.b)\n",
    "        return params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "# Generating fake data for a single input\n",
    "np.random.seed(42) # For reproducible random values\n",
    "X_fake = [Value(x) for x in np.random.randn(11)] # Simulate a single input with 11 features\n",
    "\n",
    "# Initialize the model and perform a forward pass with the fake data\n",
    "model = SimpleANN()\n",
    "try:\n",
    "    y_pred = model.forward(X_fake)\n",
    "    print(\"Forward pass successful. Output:\", y_pred)\n",
    "except TypeError as e:\n",
    "    print(\"Error during forward pass:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from micrograd.engine import Value\n",
    "\n",
    "# Binary cross entropy function for Value objects\n",
    "def binary_cross_entropy(y_pred, y_true):\n",
    "    epsilon = 1e-15\n",
    "    # Ensure y_pred values are clamped to the range [epsilon, 1-epsilon]\n",
    "    y_pred_clamped_data = max(min(y_pred.data, 1 - epsilon), epsilon)\n",
    "    y_pred_clamped = Value(y_pred_clamped_data)\n",
    "    \n",
    "    # Compute the binary cross entropy\n",
    "    loss = -(y_true * Value(math.log(y_pred_clamped.data)) + \n",
    "             (1 - y_true) * Value(math.log(1 - y_pred_clamped.data)))\n",
    "    return loss\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(0.01 * np.random.randn()) for _ in range(nin)]\n",
    "        self.b = Value(0)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # x is a list of Value objects; perform element-wise multiplication and sum\n",
    "        act = sum(wi * xi for wi, xi in zip(self.w, x)) + self.b\n",
    "        return act\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout, activation=relu):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Process each neuron's output through the activation function\n",
    "        outs = [self.activation(neuron(x)) for neuron in self.neurons]\n",
    "        return outs\n",
    "\n",
    "class SimpleANN:\n",
    "    def __init__(self):\n",
    "        self.layer1 = Layer(11, 5, activation=relu)\n",
    "        self.layer2 = Layer(5, 4, activation=relu)\n",
    "        self.output_layer = Layer(4, 1, activation=sigmoid)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Process input through each layer\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x[0]\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in [self.layer1, self.layer2, self.output_layer]:\n",
    "            for neuron in layer.neurons:\n",
    "                params += neuron.w\n",
    "                params.append(neuron.b)\n",
    "        return params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "# Assuming X_train_scaled and y_train are defined elsewhere\n",
    "X_train_scaled_values = [[Value(x) for x in sample] for sample in X_train_scaled.values]\n",
    "\n",
    "#TODO check that this is being converted properly... consider doing y_train.values \n",
    "# it could be using the header again.... \n",
    "y_train_values = [Value(y) for y in y_train]\n",
    "\n",
    "model = SimpleANN()\n",
    "epochs = 100\n",
    "lr = 0.01\n",
    "batch_size = 32\n",
    "\n",
    "def create_batches(X, y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        yield X[i:i + batch_size], y[i:i + batch_size]\n",
    "\n",
    "# Training loop corrected for handling Value objects\n",
    "for epoch in range(epochs):\n",
    "    permutation = np.random.permutation(len(X_train_scaled_values))\n",
    "    X_train_shuffled = [X_train_scaled_values[i] for i in permutation]\n",
    "    y_train_shuffled = [y_train_values[i] for i in permutation]\n",
    "    \n",
    "    batches = create_batches(X_train_shuffled, y_train_shuffled, batch_size)\n",
    "    epoch_losses = []\n",
    "    for X_batch, y_batch in batches:\n",
    "        batch_loss = 0\n",
    "        for x, y in zip(X_batch, y_batch):\n",
    "            model.zero_grad()\n",
    "            y_pred = model.forward(x)\n",
    "            loss = binary_cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            for p in model.parameters():\n",
    "                p.data -= lr * p.grad\n",
    "            batch_loss += loss.data\n",
    "        \n",
    "        epoch_losses.append(batch_loss / len(X_batch))\n",
    "    \n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# import pickle\n",
    "# model_filename = 'model-ann-11-5-4-1-micrograd.pkl'\n",
    "# model_save_path = os.path.join(saved_models_dir, model_filename)\n",
    "# with open(model_save_path, 'wb') as f:\n",
    "#     pickle.dump(model, f)\n",
    "# print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Evaluate performance on the test set\n",
    "X_test_scaled_values = [[Value(x) for x in sample] for sample in X_test_scaled] # Convert to list of Value objects\n",
    "# Create a function to predict output using our trained Micrograd model. \n",
    "# Since output layer uses a sigmoid activation, output will be in range [0, 1]. \n",
    "# As done with PyTorch, consider outputs greater than or equal to 0.5 as class 1 (positive class) and less than 0.5 as class 0 (negative class).\n",
    "def predict(model, X):\n",
    "    predictions = []\n",
    "    for x in X:\n",
    "        pred = model.forward(x)\n",
    "        pred_class = 1 if pred.data >= 0.5 else 0\n",
    "        predictions.append(pred_class)\n",
    "    return predictions\n",
    "# Calculate Performance Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Convert y_test to integer values for comparison\n",
    "y_test_int = [int(y) for y in y_test]\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = predict(model, X_test_scaled_values)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_int, y_pred)\n",
    "precision = precision_score(y_test_int, y_pred)\n",
    "recall = recall_score(y_test_int, y_pred)\n",
    "f1 = f1_score(y_test_int, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Model Evaluation Results on Test Set:\")\n",
    "print(f\" - The model's Accuracy: {accuracy:.4f}\")\n",
    "print(f\" - The model's Precision: {precision:.4f}\")\n",
    "print(f\" - The model's Recall: {recall:.4f}\")\n",
    "print(f\" - The model's F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Model Architectural Summary\n",
    "print(\"\\nModel Architectural Summary:\")\n",
    "print(\" - Input Layer: 11 units (matching the dimensionality of the input features)\")\n",
    "print(\" - First Hidden Layer: 5 neurons with ReLU activation\")\n",
    "print(\" - Second Hidden Layer: 4 neurons with ReLU activation\")\n",
    "print(\" - Output Layer: 1 neuron with Sigmoid activation (for binary classification)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install numpy scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: (10 points)\n",
    "\n",
    "* Repeat Task 3 with the following new architecture of the neural network:\n",
    "\n",
    "![Task 4 ANN architecture](figures/nn-2.png)\n",
    "\n",
    "* Input layer will still have 11 units as the dimension of training set (i.e, number of columns = 11).\n",
    "* Hidden-layer-1: 8 neurons, with relu activation\n",
    "* Hidden-layer-2: 8 neurons, with relu activation,\n",
    "* Hidden-layer-3: 8 neurons, with relu activation,\n",
    "* Output-layer: 1 neuron with sigmoid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution using Micrograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution using either PyTorch or Tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: (10 points)\n",
    "\n",
    "* Repeat Task 3 with the following new architecture of the neural network:\n",
    "\n",
    "![Task 5 ANN architecture](figures/nn-3.png)\n",
    "\n",
    "* Input layer will still have 11 units as the dimension of training set (i.e, number of columns = 11).\n",
    "* Hidden-layer-1: 8 neurons, with relu activation\n",
    "* Hidden-layer-2: 4 neurons, with relu activation,\n",
    "* Hidden-layer-3: 2 neurons, with relu activation,\n",
    "* Output-layer: 1 neuron with sigmoid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution using Micrograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution using either PyTorch or Tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all. Thanks for your work... :)\n",
    "\n",
    "Now, do the following to earn credit --\n",
    "\n",
    "0. Setting up:\n",
    "    - Make sure you actually experiment with this assignment. I would encourage (again) you to go through the possible compute resource you can use [video](https://youtu.be/XEfP9YTDdBM?si=NS0H46lOJZhI9b4H).\n",
    "    - It's always better to work in python virtual environment. Here are some resources for you to create and work in virtual environments [[win+mac+ubuntu](https://ashiskb.info/posts/2022/09/biswas/blog-1-python-venv/)][[windows+gpu](https://ashiskb.info/posts/2023/08/biswas/blog-win10-tensorflow/)][[ubuntu+gpu](https://ashiskb.info/posts/2023/08/biswas/blog-ubuntu-tensorflow/)]\n",
    "1. Please make sure to execute each cell in this jupyter notebook, and hit the 'Save' button, or go \"File > Save and Checkpoint\" menu option to save the notebook.\n",
    "2. Submit this notebook \"2024-Spring-DL-PA1-assignment.ipynb\" into Canvas \"Assignment 1\" entry. \n",
    "3. Done!\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
